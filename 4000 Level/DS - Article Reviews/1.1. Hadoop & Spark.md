### A. Hadoop
### üìò **Purpose & Overview**

- Hadoop MapReduce is a **parallel processing framework** used to process **massive datasets (terabytes)** reliably across **clusters of commodity hardware**.
    
- Data is processed in the form of **<key, value> pairs** using two main phases:
    
    - **Map**: Processes data independently.
        
    - **Reduce**: Aggregates(sum up) the output.
        

---

### üß† **Architecture & Components**

- **HDFS (Hadoop Distributed File System)** stores the input/output data.
    
- **YARN Architecture**:
    
    - **ResourceManager**: Master that manages resources.
        
    - **NodeManager**: Worker node-level agent.
        
    - **MRAppMaster**: Manages a specific MapReduce job.
        

---

### üõ†Ô∏è **MapReduce Execution Flow**

1. **Input**: File split into chunks ‚Üí map tasks.
    
2. **Map Phase**: Generates intermediate <key, value> pairs.
    
3. **Combine (optional)**: Local aggregation to reduce shuffle size.
    
4. **Shuffle and Sort**: Groups intermediate data by key.
    
5. **Reduce Phase**: Produces final output <key, value> pairs.
    
6. **Output**: Stored in HDFS.
    

---

### üíª **Programming & APIs**

- Hadoop is Java-based, but supports:
    
    - **Hadoop Streaming**: For non-Java mappers/reducers (e.g., shell, Python).
        
    - **Hadoop Pipes**: C++ API support.
        

---

### üìö **Example Application ‚Äì WordCount**

- **v1.0**: Basic implementation counting words from text input.
    
- **v2.0**: Enhanced version with:
    
    - **Case sensitivity toggling**
        
    - **Pattern filtering** using DistributedCache
        
    - **Counters** to track metrics
        

---

### üóÇÔ∏è **Input/Output**

- All inputs/outputs are handled as **<key, value>**.
    
- **TextInputFormat** and **TextOutputFormat** are defaults.
    
- Input data is split using **InputSplits** and read via **RecordReader**.
    
- Output is written using **RecordWriter**.
    

---

### üîß **Job Configuration & Tuning**

- Configure jobs via the `Job` class.
    
- Set:
    
    - Number of map/reduce tasks
        
    - Input/output paths
        
    - Mapper/Reducer/Combiner classes
        
    - Custom comparators, partitioners
        
- Tune memory, buffer sizes, and parallelism using properties like:
    
    - `mapreduce.task.io.sort.mb`
        
    - `mapreduce.reduce.shuffle.input.buffer.percent`
        

---

### üîç **Advanced Features**

- **Partitioner**: Decides how map outputs are split among reducers.
    
- **Combiner**: Local reduce function at mapper side.
    
- **Counters**: Custom counters for monitoring.
    
- **DistributedCache**: Distribute files (e.g., patterns, libraries) to tasks.
    
- **Task Environment Config**: JVM settings, memory limits.
    
- **Task Logs**: Tracked by NodeManager and stored in logs directory.
    

---

### üîó **Execution Enhancements**

- **Job Submission**: Via CLI or API (`Job.waitForCompletion()`).
    
- **Job Monitoring**: Through ResourceManager or `mapred job` command.
    
- **Job Chaining**: Use output of one job as input for another.
    

---

### üêû **Debugging, Profiling & Fault Tolerance**

- **Profiling**: Enable via `mapreduce.task.profile`.
    
- **Debug Scripts**: Execute on task failure using DistributedCache.
    
- **Bad Record Skipping**: Useful for noisy or corrupt data.
    
    - Skip using `SkipBadRecords` API.
        

---

### üóúÔ∏è **Compression**

- **Intermediate output**: Compress map output using:
    
    - `mapreduce.map.output.compress`
        
- **Final output**: Compress using:
    
    - `FileOutputFormat.setCompressOutput`
        

---

### üì¶ **Output Management**

- Output is staged in a temporary directory and committed on task success using **OutputCommitter**.
    
- Can safely manage **side-effect files** using task-specific paths.
    

---

### üßÆ **Secondary Sort, Speculative Execution, & Queues**

- **Secondary sort**: Customize grouping and sorting via comparators.
    
- **Speculative execution**: Redundant task runs to handle stragglers.
    
- **Queues**: Submit jobs to specific queues (default is 'default').
    

---

### ‚úÖ **Summary**

This document provides a **deep dive into every user-facing component** of Hadoop MapReduce‚Äîfrom basic concepts to job configuration, input/output handling, advanced optimization, and debugging techniques. It‚Äôs a **go-to manual** for both new and experienced Hadoop users.

### B. Apache Spark


### **What is Apache Spark?**

**Apache Spark** is an **open-source distributed computing system** used for **big data processing**, built for **speed, ease of use**, and **sophisticated analytics**. It was developed at **UC Berkeley‚Äôs AMPLab** in 2009 and later donated to the **Apache Software Foundation**.

Unlike Hadoop's **disk-based** MapReduce, Spark emphasizes **in-memory processing**, making it **significantly faster** for many workloads.

---

### **Key Features of Apache Spark

|Feature|Description|
|---|---|
|**In-Memory Processing**|Keeps intermediate data in RAM to avoid disk I/O. Much faster than Hadoop MapReduce.|
|**Unified Analytics Engine**|Supports batch processing, real-time streaming, machine learning (MLlib), and graph processing (GraphX).|
|**Lazy Evaluation**|Builds a DAG (Directed Acyclic Graph) of transformations for optimization.|
|**Fault Tolerance**|Recovers lost data through **lineage** (re-computation) rather than replication.|
|**Language Support**|APIs in **Scala, Java, Python, R**, and **SQL**.|
|**Runs Everywhere**|On **Hadoop (YARN)**, standalone cluster, **Kubernetes**, **Apache Mesos**, or even **cloud (AWS, GCP)**.|
|**Built-in Libraries**|MLlib (ML), Spark SQL, GraphX, Structured Streaming.|

---

### **Apache Spark Architecture

### **Key Components**:

|Component|Description|
|---|---|
|**Driver Program**|Runs the `main()` function of the application and creates **SparkContext**.|
|**Cluster Manager**|Allocates resources (e.g., YARN, Mesos, Kubernetes, Standalone).|
|**SparkContext**|Connects to the cluster manager and coordinates the job execution.|
|**Executors**|Worker nodes that run the **tasks** and hold data in memory.|
|**Tasks**|Units of work sent to executors.|
|**RDDs / DataFrames**|Abstractions representing distributed datasets.|

### **Execution Flow**:

1. User writes code using Spark APIs.
    
2. Spark creates a **DAG** of transformations.
    
3. DAG Scheduler breaks it into **stages** and **tasks**.
    
4. Tasks run on workers and results are collected.
    

---

### **Core Concepts

|Concept|Description|
|---|---|
|**RDD (Resilient Distributed Dataset)**|Immutable distributed collections of objects. Fault-tolerant.|
|**DataFrame**|Like RDD, but with named columns (like a database table). Optimized via Catalyst engine.|
|**Dataset**|Type-safe version of DataFrame (Scala/Java).|
|**Transformations**|Lazy operations like `map()`, `filter()`, `groupBy()`.|
|**Actions**|Triggers execution: `collect()`, `count()`, `saveAsTextFile()`.|
|**Lineage**|Tracks how RDDs are built from other RDDs to recover lost partitions.|

---

### **Spark vs Hadoop MapReduce

|Feature|Apache Spark|Hadoop MapReduce|
|---|---|---|
|**Performance**|Fast (in-memory)|Slower (disk I/O)|
|**Ease of Use**|Concise APIs, interactive shell|Verbose code|
|**Data Processing**|Batch, Streaming, SQL, ML|Batch only|
|**Latency**|Low (real-time possible)|High|
|**Language Support**|Scala, Python, Java, R, SQL|Java only (with Streaming/Pipes for others)|
|**Fault Tolerance**|Lineage-based|Replication-based|

---

### **Libraries in Apache Spark

|Library|Purpose|
|---|---|
|**Spark SQL**|Query structured data with SQL or DataFrames|
|**Spark Streaming / Structured Streaming**|Real-time stream processing|
|**MLlib**|Machine Learning (regression, classification, clustering, etc.)|
|**GraphX**|Graph processing (e.g., PageRank, shortest path)|
|**SparkR**|R language support|

---

## üß™ Example: Word Count in Apache Spark (Python)

```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")
text_file = sc.textFile("hdfs://path/to/input.txt")

word_counts = (
    text_file.flatMap(lambda line: line.split())
             .map(lambda word: (word, 1))
             .reduceByKey(lambda a, b: a + b)
)

word_counts.saveAsTextFile("hdfs://path/to/output")
```

---

### **Fault Tolerance & Linearizability in Spark

- Spark is **not strictly linearizable** in the distributed systems sense, but it ensures **deterministic recovery** using **lineage graphs**.
    
- In the event of failure, lost partitions are re-computed from the original data, not from replication like HDFS.
    
- For coordinated operations (e.g., structured streaming), **checkpointing** is used for fault tolerance.
    

---

### ** Where Spark Is Used

- Real-time analytics (e.g., log analysis)
    
- ETL pipelines
    
- Machine learning training pipelines
    
- Stream processing (IoT, finance)
    
- Graph analytics (e.g., social networks)
    
- Large-scale SQL workloads (e.g., SparkSQL replacing Hive)
    

---

### **Summary Table: Apache Spark at a Glance

|Category|Details|
|---|---|
|Initial Release|2009 (UC Berkeley), Apache since 2014|
|Main Language|Scala (Core), APIs in Java, Python, R, SQL|
|Data Abstractions|RDD, DataFrame, Dataset|
|Fault Tolerance|Lineage-based recovery|
|Cluster Managers|YARN, Mesos, Kubernetes, Standalone|
|Execution Type|In-memory, DAG-based|
|Use Cases|Batch, Streaming, SQL, ML, Graph|

---

Let me know if you'd like a **comparison chart with Hadoop**, or a **deep dive into Spark Streaming, MLlib, or DAG execution**!
