## **Intro** 
*Use a logging mechanism : logs are appended and they are committed only if *
#### **1. Abstract**

Raft is a [^1]consensus algorithm designed forÂ **understandability**Â while matching Paxosâ€™s efficiency. Key innovations:

- **Decomposition**Â into leader election, log replication, and safety.
- **Strong leadership**Â simplifies log management.
- **Randomized timers**Â for leader election.
- **Joint consensus**Â for safe cluster membership changes.

- It reduces the number of states to consider for better clarity.
- Raft introduces a new mechanism for cluster membership changes using overlapping majorities.

#### **2. Introduction**

- **Problem**: Paxos is complex and hard to implement, despite being the standard.
- **Goal**: Create a consensus algorithm that isÂ **easier to understand**Â without sacrificing safety or performance.
    
- **Solution**: Raft, which emphasizesÂ **intuition**Â through techniques like decomposition and reduced state space.

- *Raft improves understandability through decomposition and state space reduction.
- ==Raft has novel features like strong leader, randomized leader election, and joint consensus for membership changes.==
#### **3. Replicated State Machines**

- Consensus algorithms manageÂ **replicated logs**Â to ensure identical state machine execution across servers.
    
- **Properties**:
    - **Safety**: No incorrect results under non-Byzantine failures.
    - **Availability**: Works with majority operational servers.
    - **Timing independence**: Consistency doesnâ€™t rely on clocks.

	- Replicated logs, managed by consensus algorithms, store sequences of commands.
	- Consensus algorithms ensure log consistency despite server failures.
	- Practical consensus algorithms must ensure safety, availability, and independence from timing assumptions.

#### **4. Whatâ€™s Wrong with Paxos?**

- **Complexity**: Single-decree Paxos is hard to extend to practical systems (multi-Paxos).
    
- **Implementation gaps**: No clear roadmap for real-world use (e.g., Chubbyâ€™s "unproven protocol"). Complex & Multi-Paxos is not well-defined.

#### **5. Raft Consensus Algorithm**

##### **5.1 Basics**

- **Server states**: Leader, follower, candidate. Servers transition between states.
- **Terms**: Logical time intervals with at most one leader per term. Terms act as logical clocks to detect obsolete information.
- **RPCs**:Â `RequestVote`Â (elections) andÂ `AppendEntries`Â (log replication + heartbeats).

##### **5.2 Leader Election**

- *Raft uses a heartbeat mechanism to trigger leader election.
	- Followers become candidates if they don't receive heartbeats.
	- Candidates request votes; the first server to win a majority becomes the leader for the term.
	- Election timeouts are randomized to prevent split votes.
- **Trigger**: Followers time out without leader contact â†’ become candidates.
- **Voting**: Candidates win if they receive votes from a majority.
- **Randomized timeouts**Â prevent split votes.

##### **5.3 Log Replication**

- *The leader replicates log entries to followers, maintaining log consistency.*
- **Leader appends entries**Â to its log, replicates to followers viaÂ `AppendEntries`.
- **Commitment**: Entry is committed when replicated to a majority.
- **Log Matching Property**: Ensures consistency by checking prior entryâ€™s term/index.

##### **5.4 Safety**

- *Raft ensures that each state machine executes the same commands in the same order.*
- **Election restriction**: Only candidates with up-to-date logs can win.
- **Leader completeness**: Committed entries from past terms are preserved.
- **State Machine Safety**: All servers apply the same commands in the same order.

#### **5.5 Follower and candidate crashes**

- **Core Concept**: Raft handles follower and candidate crashes through retries.
- **Key Points**:
    - RPCs to crashed servers will fail and are retried.
    - Raft RPCs are idempotent.

#### **5.6 Timing and availability**

- **Core Concept**: Safety must not depend on timing, but availability does.
- **Key Points**:
    - Raft requires specific timing for leader election and stable leader maintenance.
    - The election timeout must be greater than the broadcast time.

#### **6. Cluster Membership Changes**

- **Joint consensus**: Transitional configuration where old and new majorities overlap.
- **Safety**: Prevents split-brain during reconfiguration.

#### **7. Log Compaction**

- *Raft uses snapshotting to prevent the log from growing indefinitely.*
- **Snapshots**: Replace committed log entries with compressed state to save space.
- **InstallSnapshot RPC**: Syncs lagging followers.

#### **8. Client Interaction**

- Clients contact the leader; followers redirect.
- **Linearizable reads**: Leaders must confirm their authority before responding.

#### **9. Evaluation**

- **Understandability**: Raft outperformed Paxos in student quizzes (33/43 scored higher).
- **Performance**: Elects leaders in ~150â€“300ms with minimal downtime.

#### **10. Related Work**

- Compared to Paxos, VR, and ZooKeeper, Raftâ€™sÂ **strong leadership**Â reduces complexity.

---

## **Task #1: Group-wise Insights**

### **Group 1: Core Logic (Raft's Mechanisms)**

#### âœ… **Terms and Elections**

- **Terms**: Logical time units that help in distinguishing stale leaders. Each term begins with an **election**.
    
- **Election Process**:
    
    - A server starts an election if it hasnâ€™t heard from a leader.
        
    - It increments its term and becomes a **candidate**.
        
    - It requests votes from other nodes.
        
    - A **majority vote** makes it the **leader** for that term.
        
- **Leader Election Safety**: At most one leader per term due to voting rules.
    

#### âœ… **Log Replication**

- Once elected, the **leader handles all client requests**.
    
- The leader appends entries to its log and sends **AppendEntries RPCs** to followers.
    
- An entry is **committed** once it is **stored on a majority** of servers.
    
- Leaders track match indices to know what has been replicated.
    

#### âœ… **Safety**

- **Leader Completeness Property**:
    
    - A leader must have all committed log entries from previous terms.
        
    - This prevents **overwriting** committed entries.
        
    - Ensures consistency across nodes, even through leadership changes.
        

---

### ğŸ”· **Group 2: Use Cases and Applications**

#### âœ… **Real-World Systems Using Raft**

- **Kubernetes**:
    
    - Uses **etcd**, which implements Raft, to manage **cluster state, secrets, and config maps**.
        
- **CockroachDB**:
    
    - A SQL-compatible, distributed database that uses Raft to **synchronize replicas of data shards**.
        
- **HashiCorp Consul**:
    
    - Manages cluster metadata using Raft for **strong consistency**.
        

#### âœ… **Why Raft Is Practical**

- **Leader-based design**:
    
    - Easy to reason about.
        
    - Simpler to implement than Paxos due to **clear leader roles**.
        
- Easier debugging and testing due to clear state transitions.
    

---

### ğŸ”· **Group 3: Analyze the Logic (Assumptions and Limitations)**

#### âœ… **Assumptions in Raft**

- **Crash Fault Tolerance (CFT)** only:
    
    - Nodes can crash or be slow but do not behave maliciously (**non-Byzantine**).
        
- **Network Conditions**:
    
    - Message loss, delay, and reordering are tolerated.
        
    - Partitions are **temporary**, and nodes eventually recover.
        

#### âš ï¸ **Identified Gaps**

- **No Byzantine Fault Tolerance (BFT)**:
    
    - Malicious nodes or data corruption are outside its model.
        
- **WAN Performance**:
    
    - Designed primarily for LANs.
        
    - Higher latency under WAN conditions due to **centralized leadership**.
        
    - Compared to **EPaxos**, which handles WAN better via decentralization.
        

---

### ğŸ”· **Group 4: Assess the Logic (Compare and Extend)**

#### âœ… **Raft vs Paxos**

- **Clarity**:
    
    - Raft is **leader-based**, whereas **Paxos uses symmetric roles** (proposers, acceptors).
        
    - Easier for engineers to **understand and implement** Raft.
        
- **Reconfiguration**:
    
    - Raft introduces **joint consensus** to safely change cluster membership.
        
    - In contrast, Paxos reconfiguration is **complex and error-prone**.
        

#### âš ï¸ **Limitations**

- **Leader Bottleneck**:
    
    - All client writes go through the leader â†’ can become a throughput bottleneck.
        
    - Load isnâ€™t as well distributed as in **leaderless** systems.
        

---

### ğŸ”· **Group 5: Create â€“ Critical Thinking and Extensions**

#### âœ… **Extensions to Raft**

- **Dynamic Timeouts**:
    
    - Adjust election timeouts based on observed network delays to reduce **split votes** and false elections.
        
- **Hybrid Consensus Models**:
    
    - Combine Raft's leadership model with **EPaxosâ€™s commutativity optimization**.
        
    - Useful in **geo-distributed** deployments where WAN latency is a concern.
        

#### âš ï¸ **Critique of Scalability**

- **Randomized election timeouts** are critical to Raftâ€™s safety.
    
- But with **large clusters** (e.g., 100+ nodes):
    
    - Risk of **frequent false elections** increases.
        
    - Leader communication overhead grows.
        
- May require **sharding**, **multi-leader** models, or combining with **gossip-based protocols** to scale.
    

---

### âœ… **Summary Diagram (Textual)**

*complete consensus*
![[Pasted image 20250518163235.png]]
![[Pasted image 20250518163301.png]]
![[Pasted image 20250518163327.png]]
![[Pasted image 20250518163351.png]]
![[Pasted image 20250518163415.png]]
![[Pasted image 20250518163434.png]]

*if a node stops and then restart : leader finds where it goes off and then leader will update that node with all the updated commits*

![[Pasted image 20250518163642.png]]
![[Pasted image 20250518163835.png]]
![[Pasted image 20250518163741.png]]


---

### **Key Takeaways for Exams**

1. **Raftâ€™s pillars**: Leader election, log replication, safety.
    
2. **Understandability techniques**: Decomposition, strong leadership, randomized timers.
    
3. **Safety mechanisms**: Election restrictions, leader completeness.
    
4. **Real-world impact**: Used in etcd, Consul, and TiDB.

[^1]: AÂ **consensus algorithm**Â ensures that multiple nodes in a distributed system agree on a single value or state, even if some nodes fail. It is critical for:
	
	- **Data consistency**Â (e.g., replicated databases like etcd).
	- **Leader election**Â (e.g., Kubernetes control plane).
	- **Fault tolerance**Â (e.g., surviving node crashes/network partitions).
	
	**Key Properties**:
	- **Agreement**: All non-faulty nodes decide on the same value.
	- **Termination**: Every request eventually completes.
	- **Fault Tolerance**: Works despite node failures (e.g., toleratesÂ `f`Â failures withÂ `2f+1`Â nodes).

## **3. Common Questions**

## **1. What is Paxos?**

### ğŸ§  **Paxos** is a **consensus algorithm** developed by **Leslie Lamport** in the late 1990s.

Its goal is to enable a **set of distributed nodes to agree on a single value**, even in the face of:

- **crashes**
    
- **message loss/delay**
    
- **network partitions**
    

### âœ… **Where it's used**:

- Core building block in **distributed databases**, **coordination systems**, and **replicated state machines** (e.g., Chubby at Google).
    

### ğŸ’¡ **How it works (simplified)**:

Paxos has three roles:

|Role|Description|
|---|---|
|**Proposer**|Proposes a value|
|**Acceptor**|Chooses a value|
|**Learner**|Learns the final agreed value|

Consensus is achieved in **two phases**:

1. **Prepare**: A proposer asks acceptors if theyâ€™re willing to consider a proposal.
    
2. **Accept**: If a majority accept, the value is chosen.
    

### âš ï¸ **Problems**:

- **Hard to understand** (the main reason Raft was developed).
    
- Difficult to extend for **cluster reconfiguration**, **log replication**, etc.
    

---

## ğŸ”· **2. What is a Split Brain Situation?**

### ğŸ’¥ **Split Brain** = **Network partition** + **no consensus** on whoâ€™s the leader or active node.

This happens when a distributed system splits into **two or more partitions**, each **thinking itâ€™s the â€œtrueâ€ leader**.

### âŒ **Dangers**:

- **Conflicting writes** (data inconsistency)
    
- **Double execution of tasks**
    
- **Corrupted state**
    

### ğŸ’¡ **Example**:

Imagine a database cluster with 3 nodes:

- A, B, C are connected.
    
- A loses contact with B and C (network split).
    
- Now:
    
    - A thinks itâ€™s the only node â†’ becomes leader.
        
    - B and C think A is gone â†’ elect a new leader.
        
- Now **two leaders** make independent decisions â†’ _Split Brain_.
    

### âœ… **How systems like Raft/Paxos handle it**:

- Require **majority (quorum)** to make any decisions.
    
- If a node doesnâ€™t see a majority, it **wonâ€™t act as leader**.
    

---

## ğŸ”· **3. How Raft Emphasizes Intuition via Decomposition & State Reduction**

Raft was designed not just to solve consensus â€” but to be **understandable**.

### ğŸ§  Raftâ€™s Design Philosophy = **Intuition First**

#### âœ… **Techniques used**:

---

### ğŸ”¹ **A. Decomposition into Subproblems**

Raft breaks the consensus problem into **independent parts**, which makes it easier to reason about:

|Subproblem|Responsibility|
|---|---|
|**Leader Election**|Who manages the cluster?|
|**Log Replication**|How to keep state consistent?|
|**Safety**|How to prevent incorrect decisions?|
|**Membership Changes**|How to safely add/remove nodes?|

ğŸ”„ This **modular thinking** makes it easier to implement, debug, and verify each part separately.

---

### ğŸ”¹ **B. Reduced State Space**

- Raft defines **explicit states** for servers:
    
    - **Leader**, **Follower**, **Candidate**
        
- Transitions are clearly defined.
    
- Each server always knows where it is and what rules it follows.
    

ğŸ§­ This makes system behavior more **predictable and traceable**.

---

### ğŸ”¹ **C. Randomized Election Timeouts**

- Prevents **election collisions**.
    
- Only one node likely to start an election â†’ simplifies reasoning.
    
- Adds a bit of randomness to avoid split-vote loops.
    

---

### ğŸ”¹ **D. Strong Leader Model**

- **All client communication** goes through the leader.
    
- This eliminates **ambiguity** of which node is allowed to act.
    
- Reduces complexity vs. Paxosâ€™s **peer-to-peer symmetry**.