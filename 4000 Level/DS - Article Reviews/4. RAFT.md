## **Intro** 
*Use a logging mechanism : logs are appended and they are committed only if *
#### **1. Abstract**

Raft is a [^1]consensus algorithm designed for **understandability** while matching Paxos’s efficiency. Key innovations:

- **Decomposition** into leader election, log replication, and safety.
- **Strong leadership** simplifies log management.
- **Randomized timers** for leader election.
- **Joint consensus** for safe cluster membership changes.

- It reduces the number of states to consider for better clarity.
- Raft introduces a new mechanism for cluster membership changes using overlapping majorities.

#### **2. Introduction**

- **Problem**: Paxos is complex and hard to implement, despite being the standard.
- **Goal**: Create a consensus algorithm that is **easier to understand** without sacrificing safety or performance.
    
- **Solution**: Raft, which emphasizes **intuition** through techniques like decomposition and reduced state space.

- *Raft improves understandability through decomposition and state space reduction.
- ==Raft has novel features like strong leader, randomized leader election, and joint consensus for membership changes.==
#### **3. Replicated State Machines**

- Consensus algorithms manage **replicated logs** to ensure identical state machine execution across servers.
    
- **Properties**:
    - **Safety**: No incorrect results under non-Byzantine failures.
    - **Availability**: Works with majority operational servers.
    - **Timing independence**: Consistency doesn’t rely on clocks.

	- Replicated logs, managed by consensus algorithms, store sequences of commands.
	- Consensus algorithms ensure log consistency despite server failures.
	- Practical consensus algorithms must ensure safety, availability, and independence from timing assumptions.

#### **4. What’s Wrong with Paxos?**

- **Complexity**: Single-decree Paxos is hard to extend to practical systems (multi-Paxos).
    
- **Implementation gaps**: No clear roadmap for real-world use (e.g., Chubby’s "unproven protocol"). Complex & Multi-Paxos is not well-defined.

#### **5. Raft Consensus Algorithm**

##### **5.1 Basics**

- **Server states**: Leader, follower, candidate. Servers transition between states.
- **Terms**: Logical time intervals with at most one leader per term. Terms act as logical clocks to detect obsolete information.
- **RPCs**: `RequestVote` (elections) and `AppendEntries` (log replication + heartbeats).

##### **5.2 Leader Election**

- *Raft uses a heartbeat mechanism to trigger leader election.
	- Followers become candidates if they don't receive heartbeats.
	- Candidates request votes; the first server to win a majority becomes the leader for the term.
	- Election timeouts are randomized to prevent split votes.
- **Trigger**: Followers time out without leader contact → become candidates.
- **Voting**: Candidates win if they receive votes from a majority.
- **Randomized timeouts** prevent split votes.

##### **5.3 Log Replication**

- *The leader replicates log entries to followers, maintaining log consistency.*
- **Leader appends entries** to its log, replicates to followers via `AppendEntries`.
- **Commitment**: Entry is committed when replicated to a majority.
- **Log Matching Property**: Ensures consistency by checking prior entry’s term/index.

##### **5.4 Safety**

- *Raft ensures that each state machine executes the same commands in the same order.*
- **Election restriction**: Only candidates with up-to-date logs can win.
- **Leader completeness**: Committed entries from past terms are preserved.
- **State Machine Safety**: All servers apply the same commands in the same order.

#### **5.5 Follower and candidate crashes**

- **Core Concept**: Raft handles follower and candidate crashes through retries.
- **Key Points**:
    - RPCs to crashed servers will fail and are retried.
    - Raft RPCs are idempotent.

#### **5.6 Timing and availability**

- **Core Concept**: Safety must not depend on timing, but availability does.
- **Key Points**:
    - Raft requires specific timing for leader election and stable leader maintenance.
    - The election timeout must be greater than the broadcast time.

#### **6. Cluster Membership Changes**

- **Joint consensus**: Transitional configuration where old and new majorities overlap.
- **Safety**: Prevents split-brain during reconfiguration.

#### **7. Log Compaction**

- *Raft uses snapshotting to prevent the log from growing indefinitely.*
- **Snapshots**: Replace committed log entries with compressed state to save space.
- **InstallSnapshot RPC**: Syncs lagging followers.

#### **8. Client Interaction**

- Clients contact the leader; followers redirect.
- **Linearizable reads**: Leaders must confirm their authority before responding.

#### **9. Evaluation**

- **Understandability**: Raft outperformed Paxos in student quizzes (33/43 scored higher).
- **Performance**: Elects leaders in ~150–300ms with minimal downtime.

#### **10. Related Work**

- Compared to Paxos, VR, and ZooKeeper, Raft’s **strong leadership** reduces complexity.

---

## **Task #1: Group-wise Insights**

### **Group 1: Core Logic (Raft's Mechanisms)**

#### ✅ **Terms and Elections**

- **Terms**: Logical time units that help in distinguishing stale leaders. Each term begins with an **election**.
    
- **Election Process**:
    
    - A server starts an election if it hasn’t heard from a leader.
        
    - It increments its term and becomes a **candidate**.
        
    - It requests votes from other nodes.
        
    - A **majority vote** makes it the **leader** for that term.
        
- **Leader Election Safety**: At most one leader per term due to voting rules.
    

#### ✅ **Log Replication**

- Once elected, the **leader handles all client requests**.
    
- The leader appends entries to its log and sends **AppendEntries RPCs** to followers.
    
- An entry is **committed** once it is **stored on a majority** of servers.
    
- Leaders track match indices to know what has been replicated.
    

#### ✅ **Safety**

- **Leader Completeness Property**:
    
    - A leader must have all committed log entries from previous terms.
        
    - This prevents **overwriting** committed entries.
        
    - Ensures consistency across nodes, even through leadership changes.
        

---

### 🔷 **Group 2: Use Cases and Applications**

#### ✅ **Real-World Systems Using Raft**

- **Kubernetes**:
    
    - Uses **etcd**, which implements Raft, to manage **cluster state, secrets, and config maps**.
        
- **CockroachDB**:
    
    - A SQL-compatible, distributed database that uses Raft to **synchronize replicas of data shards**.
        
- **HashiCorp Consul**:
    
    - Manages cluster metadata using Raft for **strong consistency**.
        

#### ✅ **Why Raft Is Practical**

- **Leader-based design**:
    
    - Easy to reason about.
        
    - Simpler to implement than Paxos due to **clear leader roles**.
        
- Easier debugging and testing due to clear state transitions.
    

---

### 🔷 **Group 3: Analyze the Logic (Assumptions and Limitations)**

#### ✅ **Assumptions in Raft**

- **Crash Fault Tolerance (CFT)** only:
    
    - Nodes can crash or be slow but do not behave maliciously (**non-Byzantine**).
        
- **Network Conditions**:
    
    - Message loss, delay, and reordering are tolerated.
        
    - Partitions are **temporary**, and nodes eventually recover.
        

#### ⚠️ **Identified Gaps**

- **No Byzantine Fault Tolerance (BFT)**:
    
    - Malicious nodes or data corruption are outside its model.
        
- **WAN Performance**:
    
    - Designed primarily for LANs.
        
    - Higher latency under WAN conditions due to **centralized leadership**.
        
    - Compared to **EPaxos**, which handles WAN better via decentralization.
        

---

### 🔷 **Group 4: Assess the Logic (Compare and Extend)**

#### ✅ **Raft vs Paxos**

- **Clarity**:
    
    - Raft is **leader-based**, whereas **Paxos uses symmetric roles** (proposers, acceptors).
        
    - Easier for engineers to **understand and implement** Raft.
        
- **Reconfiguration**:
    
    - Raft introduces **joint consensus** to safely change cluster membership.
        
    - In contrast, Paxos reconfiguration is **complex and error-prone**.
        

#### ⚠️ **Limitations**

- **Leader Bottleneck**:
    
    - All client writes go through the leader → can become a throughput bottleneck.
        
    - Load isn’t as well distributed as in **leaderless** systems.
        

---

### 🔷 **Group 5: Create – Critical Thinking and Extensions**

#### ✅ **Extensions to Raft**

- **Dynamic Timeouts**:
    
    - Adjust election timeouts based on observed network delays to reduce **split votes** and false elections.
        
- **Hybrid Consensus Models**:
    
    - Combine Raft's leadership model with **EPaxos’s commutativity optimization**.
        
    - Useful in **geo-distributed** deployments where WAN latency is a concern.
        

#### ⚠️ **Critique of Scalability**

- **Randomized election timeouts** are critical to Raft’s safety.
    
- But with **large clusters** (e.g., 100+ nodes):
    
    - Risk of **frequent false elections** increases.
        
    - Leader communication overhead grows.
        
- May require **sharding**, **multi-leader** models, or combining with **gossip-based protocols** to scale.
    

---

### ✅ **Summary Diagram (Textual)**

*complete consensus*
![[Pasted image 20250518163235.png]]
![[Pasted image 20250518163301.png]]
![[Pasted image 20250518163327.png]]
![[Pasted image 20250518163351.png]]
![[Pasted image 20250518163415.png]]
![[Pasted image 20250518163434.png]]

*if a node stops and then restart : leader finds where it goes off and then leader will update that node with all the updated commits*

![[Pasted image 20250518163642.png]]
![[Pasted image 20250518163835.png]]
![[Pasted image 20250518163741.png]]


---

### **Key Takeaways for Exams**

1. **Raft’s pillars**: Leader election, log replication, safety.
    
2. **Understandability techniques**: Decomposition, strong leadership, randomized timers.
    
3. **Safety mechanisms**: Election restrictions, leader completeness.
    
4. **Real-world impact**: Used in etcd, Consul, and TiDB.

[^1]: A **consensus algorithm** ensures that multiple nodes in a distributed system agree on a single value or state, even if some nodes fail. It is critical for:
	
	- **Data consistency** (e.g., replicated databases like etcd).
	- **Leader election** (e.g., Kubernetes control plane).
	- **Fault tolerance** (e.g., surviving node crashes/network partitions).
	
	**Key Properties**:
	- **Agreement**: All non-faulty nodes decide on the same value.
	- **Termination**: Every request eventually completes.
	- **Fault Tolerance**: Works despite node failures (e.g., tolerates `f` failures with `2f+1` nodes).

## **3. Common Questions**

## **1. What is Paxos?**

### 🧠 **Paxos** is a **consensus algorithm** developed by **Leslie Lamport** in the late 1990s.

Its goal is to enable a **set of distributed nodes to agree on a single value**, even in the face of:

- **crashes**
    
- **message loss/delay**
    
- **network partitions**
    

### ✅ **Where it's used**:

- Core building block in **distributed databases**, **coordination systems**, and **replicated state machines** (e.g., Chubby at Google).
    

### 💡 **How it works (simplified)**:

Paxos has three roles:

|Role|Description|
|---|---|
|**Proposer**|Proposes a value|
|**Acceptor**|Chooses a value|
|**Learner**|Learns the final agreed value|

Consensus is achieved in **two phases**:

1. **Prepare**: A proposer asks acceptors if they’re willing to consider a proposal.
    
2. **Accept**: If a majority accept, the value is chosen.
    

### ⚠️ **Problems**:

- **Hard to understand** (the main reason Raft was developed).
    
- Difficult to extend for **cluster reconfiguration**, **log replication**, etc.
    

---

## 🔷 **2. What is a Split Brain Situation?**

### 💥 **Split Brain** = **Network partition** + **no consensus** on who’s the leader or active node.

This happens when a distributed system splits into **two or more partitions**, each **thinking it’s the “true” leader**.

### ❌ **Dangers**:

- **Conflicting writes** (data inconsistency)
    
- **Double execution of tasks**
    
- **Corrupted state**
    

### 💡 **Example**:

Imagine a database cluster with 3 nodes:

- A, B, C are connected.
    
- A loses contact with B and C (network split).
    
- Now:
    
    - A thinks it’s the only node → becomes leader.
        
    - B and C think A is gone → elect a new leader.
        
- Now **two leaders** make independent decisions → _Split Brain_.
    

### ✅ **How systems like Raft/Paxos handle it**:

- Require **majority (quorum)** to make any decisions.
    
- If a node doesn’t see a majority, it **won’t act as leader**.
    

---

## 🔷 **3. How Raft Emphasizes Intuition via Decomposition & State Reduction**

Raft was designed not just to solve consensus — but to be **understandable**.

### 🧠 Raft’s Design Philosophy = **Intuition First**

#### ✅ **Techniques used**:

---

### 🔹 **A. Decomposition into Subproblems**

Raft breaks the consensus problem into **independent parts**, which makes it easier to reason about:

|Subproblem|Responsibility|
|---|---|
|**Leader Election**|Who manages the cluster?|
|**Log Replication**|How to keep state consistent?|
|**Safety**|How to prevent incorrect decisions?|
|**Membership Changes**|How to safely add/remove nodes?|

🔄 This **modular thinking** makes it easier to implement, debug, and verify each part separately.

---

### 🔹 **B. Reduced State Space**

- Raft defines **explicit states** for servers:
    
    - **Leader**, **Follower**, **Candidate**
        
- Transitions are clearly defined.
    
- Each server always knows where it is and what rules it follows.
    

🧭 This makes system behavior more **predictable and traceable**.

---

### 🔹 **C. Randomized Election Timeouts**

- Prevents **election collisions**.
    
- Only one node likely to start an election → simplifies reasoning.
    
- Adds a bit of randomness to avoid split-vote loops.
    

---

### 🔹 **D. Strong Leader Model**

- **All client communication** goes through the leader.
    
- This eliminates **ambiguity** of which node is allowed to act.
    
- Reduces complexity vs. Paxos’s **peer-to-peer symmetry**.