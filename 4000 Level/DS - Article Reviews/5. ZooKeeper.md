
## 1. Paper
#### **1. Abstract**

ZooKeeper is a [^1]**wait-free coordination service** for distributed systems, designed as a **kernel** for building higher-level primitives (e.g., locks, leader election). Key features:

- **FIFO client order**: Requests from a client are executed in order.
- [^2]**Linearizable writes**: All state-changing operations are serializable.
- **High performance**: Local reads enable throughput of 100K+ ops/sec for read-heavy workloads.
---
- It provides a simple, high-performance kernel for building complex coordination primitives.
- It combines aspects of group messaging, shared registers, and distributed lock services.
- It offers a wait-free interface with an event-driven mechanism.
- It guarantees FIFO client ordering and linearizability for state-changing requests.
- It's designed for high throughput, handling tens to hundreds of thousands of transactions per second.
#### **2. Introduction**

- **Problem**: Distributed systems need coordination (e.g., configuration, leader election), but custom solutions are brittle.
    
- **Solution**: ZooKeeper provides a **generic coordination kernel** with a simple API (like a filesystem) to implement custom primitives.
    
- **Key innovation**: Wait-free operations avoid blocking and improve fault tolerance.

	*It guarantees FIFO client ordering and linearizable writes.*
#### **3. ZooKeeper Service**

##### **3.1 Service Overview**

- **Data model**: Hierarchical namespace of **znodes** (similar to files/directories).
    
    - **Regular znodes**: Explicitly created/deleted (persistent).
    - **Ephemeral znodes**: Auto-deleted when the creator’s session ends (e.g., for transient clients).
    - **Sequential flag**: Appends monotonic counters (e.g., `/lock-0001`) for ordered naming.
        
- **Watches**: Clients receive one-time notifications for znode changes (no polling).

- Znodes can be created with a sequential flag to append a unique, increasing counter to their name.

##### **3.2 Client API**

Core methods:

| Method                      | Purpose                                                 |
| --------------------------- | ------------------------------------------------------- |
| `create(path, data, flags)` | Creates a znode (ephemeral/sequential).                 |
| `delete(path, version)`     | Deletes a znode if its version matches.                 |
| `exists(path, watch)`       | Checks znode existence + triggers watch on changes.     |
| `getData(path, watch)`      | Reads znode data + sets watch for updates.              |
| `sync()`                    | Ensures read-after-write consistency (linearizability). |

##### **3.3 Guarantees**

- **Linearizable writes**: All updates are totally ordered.
- **FIFO client order**: Per-client request ordering.
- **Liveness**: Available if majority of servers are operational.

##### **3.4 Primitives Built with ZooKeeper**

- **Configuration management**: Watch a znode for dynamic updates.
	- **Leader election**: [^3]Ephemeral znodes + sequencing (e.g., `/election/leader-0001`).
    
- **Locks**:
    - **Simple lock**: Herd effect (all clients watch the same znode).
    - **Improved lock**: Clients watch only the preceding znode to avoid herd effect.
- [^5]**Double barrier**: Synchronizes process start/end using child znodes under a barrier znode.
---
- ZooKeeper's wait-free nature and ordering guarantees enable efficient implementations.
- **Configuration Management**: Znodes store configuration data, and watches notify processes of updates.
- **Rendezvous**: A znode is used for processes to exchange information, such as connection details.
- **Group Membership**: Ephemeral znodes represent group members, and their presence indicates active members.
- **Simple Locks**: Znodes can implement locks; ephemeral znodes can be used to manage lock ownership.
- **Simple Locks without Herd Effect**: Sequential znodes can prevent the herd effect when a lock is released.
- **Read/Write Locks**: The lock implementation can be extended to support read/write locks.
- **Double Barrier**: Znodes can synchronize the beginning and end of a computation.

#### **4. ZooKeeper Applications**

- **Fetching Service (Yahoo! Crawler)**: Manages master-worker configuration and leader election.
- **Katta (Distributed Indexer)**: Tracks shard assignments and handles failover.
- **Yahoo! Message Broker**: Manages topic distribution and fault detection.
---
- ZooKeeper is a core component of Yahoo!'s infrastructure.
- It's used for managing configuration, naming, group membership, and synchronization.
- Examples include managing metadata for a large-scale messaging system and coordinating search crawlers.
#### **5. Implementation**

*ZooKeeper's architecture emphasizes high performance and reliability through replication.*
- **Replicated database**: In-memory data tree + disk-based write-ahead log (WAL).
- **Atomic Broadcast (Zab protocol)**: Leader-based protocol for totally ordered writes.
- **Fuzzy snapshots**: Periodic in-memory snapshots for recovery without locking.
- [^4]**Client-server interaction**:
    - Reads are served locally (no consensus).
    - Writes go through Zab (leader broadcasts updates).
---
- Servers in a ZooKeeper ensemble maintain an in-memory image of the data tree.
- A majority of servers must be available for the service to be operational.
- One server is elected as the leader to handle write requests.
- The leader uses an atomic broadcast protocol (Zab) to synchronize updates across followers.
- Read requests are served locally from each server's in-memory copy.
- ZooKeeper employs a request pipeline to process requests efficiently.
#### **6. Evaluation**

- **Throughput**:
    - Reads: 460K ops/sec (13 servers).
    - Writes: 21K ops/sec (3 servers).
        
- **Latency**: ~1.2ms for create/delete operations.
- **Fault tolerance**: Recovers within 200ms on leader failure.

#### **7. Related Work**

- **Chubby**: Similar to ZooKeeper but uses blocking locks and forces clients to connect to the leader.
- **Paxos/Raft**: ZooKeeper uses Zab (a variant of Paxos) for consensus.
- **Dynamo**: Key-value store without strong consistency guarantees.

	*kafka, Hadoop, netflix, linkedin, pinterest*

#### **8. Conclusions**

ZooKeeper’s **wait-free API** and **relaxed consistency** enable high-performance coordination. It is widely used for metadata management, leader election, and locks in distributed systems.

---

## 2. Common questions

Q1. ZooKeeper achieves high performance, especially in read-heavy applications, through several key design choices:

- **Local Reads:** ZooKeeper servers process read requests locally from their own in-memory copy of the data tree, avoiding the overhead of consensus protocols for reads. This significantly increases read throughput as reads don't have to go through the leader.
    
- **Asynchronous Operations:** ZooKeeper enables clients to submit operations asynchronously, allowing for multiple outstanding requests at a time. This pipelining of requests improves efficiency and reduces latency.
    
- **Wait-free API:** ZooKeeper's API is wait-free, meaning that operations do not block waiting for other clients, which could be slow or faulty. This design choice prevents slow clients from hindering the performance of faster ones.
    
- **Client Caching and Watches:** ZooKeeper allows clients to cache data and uses a watch mechanism to notify clients of changes. This reduces the need for repeated reads, further enhancing performance.
    

Q2. We can have polling based or event based monitoring strategies right. so what are they and how they different or similar with the zoo keeper?

**Polling**

- **Concept**: In polling, a client repeatedly asks a server (or another system) for information at regular intervals. The client actively "polls" the server to check for updates.
    
- **Characteristics**:
    - **Periodic checks:** The client decides how often to check.
    - **Client-driven:** The client initiates the communication.
        
    - **Simple to implement (sometimes):** It's often straightforward to write a loop that repeatedly calls a "get data" function.
        
    - **Resource-intensive (potentially):** If the polling interval is too short, the client and server waste resources checking for updates when nothing has changed.
        
    - **Latency:** There's always a delay (latency) between when an update occurs and when the client detects it, determined by the polling interval.
        

**Event-Based Monitoring**

- **Concept**: In event-based monitoring, the server (or system being monitored) notifies the client when something of interest happens. The client reacts to "events" pushed to it by the server.
- **Characteristics**:
    - **Reactive:** The client only takes action when notified.
    - **Server-driven:** The server initiates the communication (for notifications).
    - **Efficient (potentially):** Can be very efficient since resources aren't wasted checking for changes that haven't occurred.
    - **Lower latency (potentially):** Clients can react to changes very quickly.
        
    - **More complex to implement (sometimes):** Requires a mechanism for the server to track events and notify clients (e.g., callbacks, webhooks, message queues).

**How ZooKeeper Relates**

ZooKeeper primarily uses an **event-based mechanism** through its **watch** feature. This is a crucial aspect of its design:

- **ZooKeeper's Watches:**
    - A client can set a watch on a znode.
    - If the data in that znode changes, or if the znode itself is created or deleted, ZooKeeper notifies the client.
    - This is event-based because ZooKeeper _pushes_ the notification to the client when the event occurs, rather than the client having to repeatedly ask ("poll").

**Comparison**

Here's how ZooKeeper's watch mechanism compares to polling:

- **Efficiency:** ZooKeeper's watches are much more efficient than polling for many use cases. Clients don't waste resources constantly asking "has it changed? has it changed?". They only receive notifications when there's an actual change.
- **Latency:** Watches can provide lower latency. A client can be notified very quickly when a znode is updated, compared to polling where the client might have to wait until its next polling interval to discover the change.
- **Server Load:** Watches reduce the load on the ZooKeeper servers. The servers don't have to respond to a high volume of polling requests.

**Why ZooKeeper Uses Watches**

ZooKeeper is designed for coordination in distributed systems, where timely notification of changes is often critical. For example:

- If a process needs to know immediately when a configuration value has been updated, polling would introduce delays. A watch ensures that the process reacts to the new configuration as soon as it's available.
    
- In leader election, processes might watch for the creation or deletion of specific znodes to determine who the current leader is. Watches enable quick responses to leader changes.
    

**Important Note:** ZooKeeper's watches are one-time triggers. If a client needs to be notified of _every_ change to a znode, it must re-register the watch after each notification. This adds some complexity, as noted in the critique earlier.

---
## **Group-wise Insights**

#### **Group 1: Logic of the Paper**

- **Zab protocol**:
    
    - Ensures that updates to ZooKeeper are done in a consistent order across all servers.
    - It's a leader-based atomic broadcast protocol, meaning the leader server is responsible for proposing changes, and the protocol ensures that either all servers apply the change or none do.
    - "Linearizable writes" means that updates appear to happen instantaneously and in a total order, as if there were only one single copy of the data.
        
- **Session management**:
    
    - ZooKeeper clients establish sessions with the ZooKeeper service.
    - Ephemeral znodes are a special type of znode that is automatically deleted when the client's session ends (either intentionally or due to a failure).
    - This feature is very useful for things like group membership – if a client/process dies, its ephemeral znodes disappear, and other processes know it's gone.
        
- **Watch mechanism**:
    
    - Allows clients to get notifications when a znode changes.
    - Instead of clients constantly polling ZooKeeper to see if something has changed, they can set a "watch" and ZooKeeper will tell them when it does.
    - This significantly reduces the load on the ZooKeeper servers and the network.
        

**Group 2: Use Cases and Applications**

- **Kafka**:
    
    - Kafka, a distributed streaming platform, uses ZooKeeper to manage the cluster of Kafka brokers.
    - ZooKeeper helps with things like leader election for partitions and broker discovery.
- **Hadoop**:
    
    - Hadoop, a framework for distributed processing of large datasets, uses ZooKeeper for High Availability (HA) of the HDFS (Hadoop Distributed File System) NameNode.
    - ZooKeeper enables automatic failover if the primary NameNode fails.
- **Primitives**:
    
    - ZooKeeper provides a foundation for implementing various distributed coordination primitives.
    - These are not built into ZooKeeper itself but are implemented by clients using ZooKeeper's API.
    - Examples include:
        - **Locks**: To control access to shared resources.
        - **Barriers**: To make sure that a group of processes all reach a certain point before proceeding.
        - **Leader election**: To choose one process as the leader in a distributed system.
            

**Group 3: Analyze the Logic – Reasoning**

- **Assumptions**:
    
    - **Crash-stop failures**: ZooKeeper assumes that when a server fails, it simply stops working (crashes) and doesn't exhibit arbitrary or malicious behavior (Byzantine faults).
    - **Quorum**: ZooKeeper requires a majority of servers to be functioning to provide service. This ensures consistency; if a majority isn't available, the system becomes unavailable.
        
- **Gaps**:
    
    - **No built-in security**: The base ZooKeeper system doesn't include strong security features. This means that, without additional security measures, malicious clients could potentially disrupt the service (e.g., by creating a large number of znodes).
    - **Write throughput limitations**: Because Zab ensures that writes are linearizable, the leader server has to serialize (order) these writes. This can limit the overall write throughput of the system, as all writes essentially go through a single point.
        

**Group 4: Assess the Logic – Add Your Knowledge**

- **Comparison to Raft**:
    
    - ZooKeeper's Zab and Raft are both consensus algorithms, and they share similar goals.
    - Zab is optimized for pipelining, which can improve performance.
    - ZooKeeper relaxes consistency guarantees for reads to improve read performance. Reads in ZooKeeper are served from local replicas, while Raft always goes through the leader for strong consistency.
        
- **Limitation**:
    
    - ZooKeeper's design doesn't include native support for replicating data across geographically distant data centers. This adds complexity to disaster recovery and can increase latency if not handled carefully.

**Group 5: Create – Critical Thinking**

- **Extensions**:
    
    - **Geo-replication**: To improve fault tolerance and availability, ZooKeeper could be extended to better support replication across multiple data centers. This would likely involve modifications or variations of the Zab protocol.
    - **Byzantine fault tolerance (BFT)**: To make ZooKeeper more robust in environments where servers might exhibit malicious behavior, BFT protocols could be integrated. This would add complexity but increase security.
- **Critique**:
    
    - The watch mechanism, while efficient, has a limitation: watches are one-time triggers. If a client needs to be continuously notified of changes, it has to re-register the watch after every notification. This can make client code more complex, especially when dealing with frequent updates.
    

---

### **Key Takeaways for Exams**

1. **ZooKeeper’s pillars**: Wait-free API, Zab consensus, ephemeral znodes.
    
2. **Consistency model**: Linearizable writes + FIFO client order.
    
3. **Performance**: Local reads enable high throughput; writes are slower due to Zab.
    
4. **Real-world impact**: Used in Kafka, Hadoop, and Kubernetes for coordination.

[^1]: clients never block indefinitely waiting for a response. It guarantees:
	
	- **Liveness**: Clients always receive a reply (success/failure) without deadlocks.
	- **Use Case**: Acts as a "kernel" for building higher-level primitives (e.g., locks, queues).

[^2]: - All state-changing operations (writes) are **serializable** and appear to execute in a global order.
	- Ensures strong consistency: After a write completes, all subsequent reads see the updated value.
	
	How it works
	
	 **Non-blocking Operations**: Clients proceed regardless of server delays/failures.
	    
	- **Guarantees**:
	    - **Termination**: Every request completes (even if some servers fail).
	    - **Progress**: No client starves due to others’ actions.

[^3]: - **Definition**: Temporary znodes tied to a client session.
	- **Use Cases**:
	    - Detecting client failures (e.g., if a client crashes, its ephemeral znodes vanish).
	    - Implementing leader election (e.g., `/election/leader` as ephemeral).

[^4]: 1. **Reads**:
	    - Served **locally** by any server (no consensus needed).
	    - Fast but may be stale (unless `sync()` is called for linearizability).
	        
	2. **Writes**:
	    - Must go through the **leader** using **Zab** (ZooKeeper Atomic Broadcast).
	    - Leader proposes updates → followers agree via consensus → commit to log.
	    - Ensures **strong consistency** (all writes are linearizable).
	
	**Why?**
	- **Reads**: Optimized for speed (no coordination).
	- **Writes**: Sacrifice speed for consistency (require global agreement)

[^5]: - **Purpose**: Synchronizes processes to start/end only when all members are ready.
	    
	- **Mechanism**:
	    1. Processes create child znodes under a barrier znode (e.g., `/barrier/member1`).
	    2. Barrier triggers when expected children count is reached.
